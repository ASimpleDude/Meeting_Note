# ============================================================
# ğŸ“ api/services/chat_service.py
# ============================================================
import logging
import numpy as np
import chromadb
from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_exception_type
from openai import AzureOpenAI, OpenAI, APIError, RateLimitError, APITimeoutError
from sentence_transformers import SentenceTransformer, CrossEncoder
from api.services.chroma_client import get_chroma_collection

from api.config.config import (
    AZURE_OPENAI_API_KEY,
    AZURE_OPENAI_API_VERSION,
    AZURE_OPENAI_ENDPOINT,
)
from api.services.moderation_service import moderate_input

# ============================================================
# âš™ï¸ Setup Logging
# ============================================================
logger = logging.getLogger(__name__)

# ============================================================
# ğŸ§  ChromaDB Client
# ============================================================
collection = get_chroma_collection()

# ============================================================
# ğŸ”§ Embedding & Reranker Models
# ============================================================
# Local embedder (miá»…n phÃ­)
local_embedder = SentenceTransformer("sentence-transformers/multi-qa-MiniLM-L6-cos-v1")

# Reranker (váº«n giá»¯ nguyÃªn)
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# OpenAI embedding client (tÃ¹y chá»n)
openai_client = OpenAI(api_key=AZURE_OPENAI_API_KEY)

# ============================================================
# ğŸ¤– Azure OpenAI Client
# ============================================================
client = AzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    api_version=AZURE_OPENAI_API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
)

# ============================================================
# ğŸ§© Embedding Helper
# ============================================================
def safe_get_embedding(query: str):
    query = query.strip()
    if not query:
        return None
    try:
        return get_embedding(query)
    except Exception as e:
        logger.error(f"âŒ Embedding failed: {e}")
        return None


def get_embedding(text: str, use_openai: bool = False):
    """
    Sinh vector embedding tá»« text.
    - Náº¿u use_openai=True â†’ dÃ¹ng text-embedding-3-small (OpenAI API)
    - NgÆ°á»£c láº¡i â†’ dÃ¹ng local model multi-qa-MiniLM-L6-cos-v1
    """
    if use_openai:
        try:
            response = openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            emb = response.data[0].embedding
            return emb
        except Exception as e:
            logger.warning(f"âš ï¸ Lá»—i khi gá»i OpenAI embedding API, fallback sang local: {e}")
            return local_embedder.encode(text, convert_to_numpy=True).tolist()
    else:
        return local_embedder.encode(text, convert_to_numpy=True).tolist()

# ============================================================
# ğŸ’¾ LÆ°u há»™i thoáº¡i vÃ o ChromaDB
# ============================================================
def save_to_chroma(session_id: str, user_message: str, assistant_reply: str):
    """LÆ°u 1 lÆ°á»£t há»™i thoáº¡i vÃ o ChromaDB."""
    text = f"[{session_id}] User: {user_message}\nAssistant: {assistant_reply}"
    embedding = get_embedding(text)
    all_ids = collection.get()["ids"]
    next_id = f"{session_id}_{len(all_ids)}"

    collection.add(
        documents=[text],
        embeddings=[embedding],
        metadatas=[{"session_id": session_id}],
        ids=[next_id],
    )
    logger.info(f"ğŸ’¾ Saved conversation to Chroma (ID: {next_id})")

# ============================================================
# ğŸ” TÃ¬m kiáº¿m thÃ´ng tin tá»« trÃ­ nhá»› (ChromaDB + Reranker)
# ============================================================
import numpy as np
from collections import defaultdict
from numpy.linalg import norm

def cosine_similarity(v1, v2):
    v1, v2 = np.array(v1), np.array(v2)
    if norm(v1) == 0 or norm(v2) == 0:
        return 0.0
    return np.dot(v1, v2) / (norm(v1) * norm(v2))

def search_memory(session_id: str, query: str, top_k: int = 5, threshold: float = 0.7, return_score=False):
    """
    Search memory with exact question match first.
    1. Check session cache
    2. Check DB for exact question previously asked â†’ return old answer
    3. Embedding search + reranker if no exact match
    4. Return only if score >= threshold
    """
    cache_key = query.strip()

    query_emb = safe_get_embedding(query)
    if not query_emb:
        return ("", 0.0) if return_score else ""

    try:
        results = collection.query(
            query_embeddings=[query_emb],
            n_results=top_k,
            where={"session_id": session_id},
            include=["documents", "embeddings"]  # cáº§n include embeddings náº¿u cÃ³
        )
    except ValueError:
        return ("", 0.0) if return_score else ""

    candidate_docs = results.get("documents", [[]])[0]
    if not candidate_docs:
        return ("", 0.0) if return_score else ""

    # =============================
    # 1ï¸âƒ£ Náº¿u embeddings cÃ³ sáºµn â†’ tÃ­nh cosine similarity
    # =============================
    candidate_embeddings = results.get("embeddings", [[]])[0]
    if candidate_embeddings is not None and len(candidate_embeddings) == len(candidate_docs):
        scores = [cosine_similarity(query_emb, doc_emb) for doc_emb in candidate_embeddings]
    else:
        # Fallback reranker
        pairs = [[query, doc] for doc in candidate_docs]
        raw_scores = reranker.predict(pairs)
        scores = [np.tanh(s) for s in raw_scores]

    # Chá»n document tá»‘t nháº¥t
    sorted_indices = np.argsort(scores)[::-1]
    best_doc = candidate_docs[sorted_indices[0]]
    best_score = scores[sorted_indices[0]]

    logger.info(f"ğŸ” Memory search top score: {best_score:.3f}")

    if best_score >= threshold:
        if return_score:
            return best_doc, best_score
        return best_doc
    else:
        return ("", best_score) if return_score else ""


# ============================================================
# ğŸ” Retry Wrapper cho Azure API
# ============================================================
@retry(
    retry=retry_if_exception_type((RateLimitError, APIError, APITimeoutError)),
    wait=wait_random_exponential(min=1, max=30),
    stop=stop_after_attempt(3),
)
def _call_azure_openai(messages: list):
    """Gá»i Azure OpenAI ChatCompletion."""
    logger.info("ğŸ”„ Gá»­i request Ä‘áº¿n Azure OpenAI...")
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.3,
        max_tokens=800,
        timeout=30,
    )
    logger.info("âœ… Nháº­n pháº£n há»“i thÃ nh cÃ´ng tá»« Azure OpenAI.")
    return response

# ============================================================
# ğŸ’¬ HÃ m chÃ­nh: Gá»i Azure OpenAI, káº¿t há»£p vá»›i Chroma Memory
# ============================================================
def generate_summary(messages: list, user_input: str = None, memory_context: str = None) -> str:
    """
    Gá»i Azure OpenAI Ä‘á»ƒ sinh pháº£n há»“i.
    Náº¿u cÃ³ memory_context thÃ¬ append vÃ o prompt trÆ°á»›c khi gá»­i.
    """
    try:
        user_message = user_input or messages[-1]["content"]

        # ThÃªm trÃ­ nhá»› náº¿u cÃ³
        if memory_context:
            user_message += f"\n\nDÆ°á»›i Ä‘Ã¢y lÃ  thÃ´ng tin liÃªn quan tá»« cÃ¡c láº§n trao Ä‘á»•i trÆ°á»›c:\n{memory_context}\n"

        temp_messages = messages.copy()
        temp_messages[-1]["content"] = user_message

        # # Báº­t láº¡i khi cáº§n kiá»ƒm duyá»‡t
        # if not moderate_input(user_message):
        #     return "ğŸš« Ná»™i dung bá»‹ tá»« chá»‘i â€” vui lÃ²ng khÃ´ng gá»­i dá»¯ liá»‡u nháº¡y cáº£m."

        response = _call_azure_openai(temp_messages)
        if not response or not response.choices:
            return "âš ï¸ KhÃ´ng cÃ³ pháº£n há»“i tá»« mÃ´ hÃ¬nh."

        reply = response.choices[0].message.content.strip()
        logger.info("âœ… Model tráº£ vá» pháº£n há»“i há»£p lá»‡.")
        return reply

    except Exception as e:
        logger.exception(f"âŒ Lá»—i khi gá»i Azure OpenAI: {e}")
        return "âš ï¸ Lá»—i khi xá»­ lÃ½ yÃªu cáº§u tá»« mÃ´ hÃ¬nh."