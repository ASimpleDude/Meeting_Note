# ============================================================
# ğŸ“ api/services/chat_service.py
# ============================================================

import logging
import re
from collections import defaultdict

import chromadb
import numpy as np
from numpy.linalg import norm
from scipy.spatial.distance import cosine
from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_exception_type
from openai import AzureOpenAI, OpenAI, APIError, RateLimitError, APITimeoutError
from sentence_transformers import SentenceTransformer, CrossEncoder

from api.services.chroma_client import get_chroma_collection
from api.config.config import (
    AZURE_OPENAI_API_KEY,
    AZURE_OPENAI_API_VERSION,
    AZURE_OPENAI_ENDPOINT,
)
from api.services.moderation_service import moderate_input


# ============================================================
# âš™ï¸ Cáº¥u hÃ¬nh logging
# ============================================================
logger = logging.getLogger(__name__)


# ============================================================
# ğŸ§  Khá»Ÿi táº¡o cÃ¡c client vÃ  model
# ============================================================
collection = get_chroma_collection()  # Káº¿t ná»‘i Ä‘áº¿n ChromaDB

# MÃ´ hÃ¬nh embedding cá»¥c bá»™ (nháº¹, miá»…n phÃ­)
local_embedder = SentenceTransformer("sentence-transformers/multi-qa-MiniLM-L6-cos-v1")

# MÃ´ hÃ¬nh reranker dÃ¹ng cho fallback
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# Client OpenAI (sá»­ dá»¥ng khi cáº§n embedding tá»« API)
openai_client = OpenAI(api_key=AZURE_OPENAI_API_KEY)

# Client Azure OpenAI (dÃ¹ng Ä‘á»ƒ sinh pháº£n há»“i há»™i thoáº¡i)
client = AzureOpenAI(
    api_key=AZURE_OPENAI_API_KEY,
    api_version=AZURE_OPENAI_API_VERSION,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
)


# ============================================================
# ğŸ”§ HÃ m tiá»‡n Ã­ch: Sinh embedding an toÃ n
# ============================================================
def get_embedding(text: str, use_openai: bool = False):
    """
    Sinh vector embedding tá»« vÄƒn báº£n.

    - Náº¿u use_openai=True â†’ sá»­ dá»¥ng OpenAI API (text-embedding-3-small)
    - NgÆ°á»£c láº¡i â†’ sá»­ dá»¥ng mÃ´ hÃ¬nh cá»¥c bá»™ (multi-qa-MiniLM-L6-cos-v1)
    """
    if use_openai:
        try:
            response = openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            logger.warning(f"Lá»—i khi gá»i OpenAI embedding API, fallback sang local: {e}")
            return local_embedder.encode(text, convert_to_numpy=True).tolist()
    else:
        return local_embedder.encode(text, convert_to_numpy=True).tolist()


def safe_get_embedding(query: str):
    """HÃ m sinh embedding cÃ³ xá»­ lÃ½ ngoáº¡i lá»‡."""
    query = query.strip()
    if not query:
        return None
    try:
        return get_embedding(query)
    except Exception as e:
        logger.error(f"Lá»—i khi sinh embedding: {e}")
        return None


# ============================================================
# ğŸ’¾ LÆ°u há»™i thoáº¡i vÃ o ChromaDB
# ============================================================
def save_to_chroma(session_id: str, user_message: str, assistant_reply: str):
    """
    LÆ°u má»™t lÆ°á»£t há»™i thoáº¡i (user + assistant) vÃ o ChromaDB.
    """
    text = f"[{session_id}] User: {user_message}\nAssistant: {assistant_reply}"
    embedding = get_embedding(text)
    all_ids = collection.get()["ids"]
    next_id = f"{session_id}_{len(all_ids)}"

    collection.add(
        documents=[text],
        embeddings=[embedding],
        metadatas=[{"session_id": session_id}],
        ids=[next_id],
    )
    logger.info(f"ÄÃ£ lÆ°u há»™i thoáº¡i vÃ o Chroma (ID: {next_id})")


# ============================================================
# ğŸ” HÃ m há»— trá»£ tÃ¬m kiáº¿m trong trÃ­ nhá»› (ChromaDB)
# ============================================================
def extract_qa_from_doc(doc: str):
    """
    TÃ¡ch pháº§n cÃ¢u há»i (User) vÃ  cÃ¢u tráº£ lá»i (Assistant) tá»« ná»™i dung doc.
    Tráº£ vá» tuple (question, answer).
    """
    user_match = re.search(r"User:\s*(.+?)(?:\n|$)", doc, re.DOTALL)
    assistant_match = re.search(r"Assistant:\s*(.+)", doc, re.DOTALL)

    question = user_match.group(1).strip() if user_match else ""
    answer = assistant_match.group(1).strip() if assistant_match else ""
    return question, answer


def cosine_similarity(vec1, vec2):
    """TÃ­nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cosine giá»¯a hai vector."""
    if vec1 is None or vec2 is None:
        return 0.0
    vec1, vec2 = np.array(vec1), np.array(vec2)
    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:
        return 0.0
    return 1 - cosine(vec1, vec2)


def search_memory(session_id: str, query: str, top_k: int = 3, threshold: float = 0.7, return_score=False):
    """
    TÃ¬m kiáº¿m trong trÃ­ nhá»› há»™i thoáº¡i (ChromaDB).
    - Chá»‰ so sÃ¡nh pháº§n cÃ¢u há»i cá»§a User.
    - Náº¿u cÃ¢u há»i trÃ¹ng khá»›p (score â‰¥ 0.9), láº¥y láº¡i cÃ¢u tráº£ lá»i cÅ© cá»§a Assistant.
    - Náº¿u khÃ´ng, so sÃ¡nh báº±ng cosine vÃ  tráº£ vá» káº¿t quáº£ tá»‘t nháº¥t.
    """
    query_emb = safe_get_embedding(query)
    if not query_emb:
        return ("", 0.0) if return_score else ""

    try:
        results = collection.query(
            query_embeddings=[query_emb],
            n_results=top_k,
            where={"session_id": session_id},
            include=["documents"]
        )
    except ValueError:
        return ("", 0.0) if return_score else ""

    candidate_docs = results.get("documents", [[]])[0]
    if not candidate_docs:
        return ("", 0.0) if return_score else ""

    best_doc = ""
    best_score = 0.0

    # So sÃ¡nh tá»«ng doc dá»±a trÃªn cÃ¢u há»i cá»§a User
    for doc in candidate_docs:
        user_q, ai_ans = extract_qa_from_doc(doc)
        if not user_q or not ai_ans:
            continue

        user_q_emb = safe_get_embedding(user_q)
        if not user_q_emb:
            continue

        sim = cosine_similarity(query_emb, user_q_emb)

        if sim > best_score:
            best_score = sim
            best_doc = ai_ans if sim >= 0.9 else doc  # Náº¿u trÃ¹ng cao, chá»‰ láº¥y cÃ¢u tráº£ lá»i

    if best_score >= threshold:
        return (best_doc, float(best_score)) if return_score else best_doc
    else:
        return ("", float(best_score)) if return_score else ""


# ============================================================
# ğŸ” HÃ m gá»i Azure OpenAI (cÃ³ retry tá»± Ä‘á»™ng)
# ============================================================
@retry(
    retry=retry_if_exception_type((RateLimitError, APIError, APITimeoutError)),
    wait=wait_random_exponential(min=1, max=30),
    stop=stop_after_attempt(3),
)
def _call_azure_openai(messages: list):
    """
    Gá»­i yÃªu cáº§u Ä‘áº¿n Azure OpenAI Ä‘á»ƒ sinh pháº£n há»“i há»™i thoáº¡i.
    CÃ³ cÆ¡ cháº¿ retry khi bá»‹ lá»—i táº¡m thá»i (RateLimit, Timeout, APIError).
    """
    logger.info("Gá»­i request Ä‘áº¿n Azure OpenAI...")
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.3,
        max_tokens=800,
        timeout=30,
    )
    logger.info("Nháº­n pháº£n há»“i thÃ nh cÃ´ng tá»« Azure OpenAI.")
    return response


# ============================================================
# ğŸ’¬ HÃ m chÃ­nh: Sinh pháº£n há»“i há»™i thoáº¡i (káº¿t há»£p memory)
# ============================================================
def generate_summary(messages: list, user_input: str = None, memory_context: str = None) -> str:
    """
    Sinh pháº£n há»“i há»™i thoáº¡i tá»« Azure OpenAI.
    Náº¿u cÃ³ 'memory_context' thÃ¬ ná»‘i thÃªm vÃ o prompt Ä‘á»ƒ cung cáº¥p ngá»¯ cáº£nh.
    """
    try:
        user_message = user_input or messages[-1]["content"]

        # ThÃªm pháº§n trÃ­ nhá»› trÆ°á»›c Ä‘Ã³ náº¿u cÃ³
        if memory_context:
            user_message += f"\n\nThÃ´ng tin liÃªn quan tá»« cÃ¡c láº§n trao Ä‘á»•i trÆ°á»›c:\n{memory_context}\n"

        temp_messages = messages.copy()
        temp_messages[-1]["content"] = user_message

        # (Tuá»³ chá»n) Kiá»ƒm duyá»‡t ná»™i dung ngÆ°á»i dÃ¹ng
        # if not moderate_input(user_message):
        #     return "Ná»™i dung bá»‹ tá»« chá»‘i â€” vui lÃ²ng khÃ´ng gá»­i dá»¯ liá»‡u nháº¡y cáº£m."

        response = _call_azure_openai(temp_messages)
        if not response or not response.choices:
            return "KhÃ´ng cÃ³ pháº£n há»“i tá»« mÃ´ hÃ¬nh."

        reply = response.choices[0].message.content.strip()
        logger.info("Model tráº£ vá» pháº£n há»“i há»£p lá»‡.")
        return reply

    except Exception as e:
        logger.exception(f"Lá»—i khi gá»i Azure OpenAI: {e}")
        return "ÄÃ£ xáº£y ra lá»—i khi xá»­ lÃ½ yÃªu cáº§u tá»« mÃ´ hÃ¬nh."
